{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c844881e",
   "metadata": {},
   "source": [
    "# Sentinel Devices - Anomaly Detection Project\n",
    "\n",
    "ISYE / CSE / MGT 6748\n",
    "\n",
    "Student: Tung Nguyen, tnguyen844@gatech.edu\n",
    "\n",
    "This notebook was run on: Python 3.13, macOS 15 Sequoia, Macbook Pro (M1 Max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394016c",
   "metadata": {},
   "source": [
    "## Database Setup & Model Training\n",
    "\n",
    "Before running this Jupyter Notebook to set up and train the anomaly detection models, set up the Python environment using the commands below:\n",
    "\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv ./sentinel-devices\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Optional: Upgrade pip\n",
    "pip install --upgrade pip\n",
    "```\n",
    "\n",
    "Then set the kernel to the python environment that was just set up.\n",
    "\n",
    "After running this notebook, there should be:\n",
    "* A SQLite3 database with training data, prediction data fully set up in the `02-data` folder\n",
    "* A ChromaDB database with the MetroPT Research Paper loaded in the `02-data` folder\n",
    "* 8 trained Prophet models in the `03-models` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653672bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from prophet import Prophet\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "load_dotenv(r'../.env');\n",
    "print('OPENAI_API_KEY' in os.environ)\n",
    "\n",
    "analog_sensors = [\n",
    "    'tp2',\n",
    "    'tp3',\n",
    "    'h1',\n",
    "    'dv_pressure',\n",
    "    'reservoirs',\n",
    "    'oil_temperature',\n",
    "    'flowmeter',\n",
    "    'motor_current'\n",
    "]\n",
    "\n",
    "digital_sensors = [\n",
    "    'comp',\n",
    "    'dv_electric',\n",
    "    'towers',\n",
    "    'mpg',\n",
    "    'lps',\n",
    "    'pressure_switch',\n",
    "    'oil_level',\n",
    "    'caudal_impulses'\n",
    "]\n",
    "\n",
    "gps_sensors = [\n",
    "    'gps_long',\n",
    "    'gps_lat',\n",
    "    'gps_speed',\n",
    "    'gps_quality'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426b87b",
   "metadata": {},
   "source": [
    "## Global Settings\n",
    "\n",
    "This section controls how much training data is used, and which specific part of the training data is used.\n",
    "\n",
    "This section also contains global model parameters that control how anomalies are flagged and consolidated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime(2022, 2, 28, 2, 0, 0) # Simulate \"today\" as Feb 27th\n",
    "\n",
    "training_window = 30*2 # days\n",
    "inference_window = 7   # days\n",
    "\n",
    "n_signal_threshold = 5     # How many sensors need to be out-of-bounds at the same time for the model to flag the timestamp as an anomaly?\n",
    "out_of_bounds_buffer = 0.1 # How far out-of-bounds does the signal need to be before its flagged as an anomaly?\n",
    "\n",
    "event_consolidation_threshold_secs = 10 # Max # of seconds between consecutive events allowed, otherwise, the two events are conslidated into one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46099014",
   "metadata": {},
   "source": [
    "### Calculated Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf56786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start: 2021-12-30 02:00:00 - 2022-02-28 02:00:00\n",
      "Inference Period: 2022-02-21 02:00:00 - 2022-02-28 02:00:00\n"
     ]
    }
   ],
   "source": [
    "train_start = (today - timedelta(days=training_window)).strftime(r'%Y-%m-%d %H:%M:%S')\n",
    "inference_start = (today - timedelta(days=inference_window)).strftime(r'%Y-%m-%d %H:%M:%S')\n",
    "end = today.strftime(r'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f'Train Start: {train_start} - {end}')\n",
    "print(f'Inference Period: {inference_start} - {end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a673a",
   "metadata": {},
   "source": [
    "## Create SQLite Database\n",
    "Load data from CSV into a SQLite3 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c9ccc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 s, sys: 6.38 s, total: 47.4 s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Column name remapping\n",
    "config = [\n",
    "    ('timestamp', 'ts'),\n",
    "    ('TP2', 'tp2'),\n",
    "    ('TP3', 'tp3'),\n",
    "    ('H1', 'h1'),\n",
    "    ('DV_pressure', 'dv_pressure'),\n",
    "    ('Reservoirs', 'reservoirs'),\n",
    "    ('Oil_temperature', 'oil_temperature'),\n",
    "    ('Flowmeter', 'flowmeter'),\n",
    "    ('Motor_current', 'motor_current'),\n",
    "    ('COMP', 'comp'),\n",
    "    ('DV_eletric', 'dv_electric'),\n",
    "    ('Towers', 'towers'),\n",
    "    ('MPG', 'mpg'),\n",
    "    ('LPS', 'lps'),\n",
    "    ('Pressure_switch', 'pressure_switch'),\n",
    "    ('Oil_level', 'oil_level'),\n",
    "    ('Caudal_impulses', 'caudal_impulses'),\n",
    "    ('gpsLong', 'gps_long'),\n",
    "    ('gpsLat', 'gps_lat'),\n",
    "    ('gpsSpeed', 'gps_speed'),\n",
    "    ('gpsQuality', 'gps_quality')\n",
    "]\n",
    "\n",
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "\n",
    "    # Load data from CSV -> SQLite\n",
    "    for csv_file in ['./raw-data/train_data_part1.csv', './raw-data/train_data_part2.csv']:\n",
    "        data = pd.read_csv(csv_file, parse_dates=['timestamp'], chunksize=100_000)\n",
    "        for chunk in data:\n",
    "            chunk.to_sql('train_data', con, if_exists='append', index=False)\n",
    "\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Rename columns\n",
    "    for c in config:\n",
    "        cur.execute(f'alter table train_data rename column {c[0]} to {c[1]};')\n",
    "    \n",
    "    # Add indices & columns\n",
    "    cur.execute('create index if not exists idx_ts on train_data(ts)')\n",
    "    cur.execute('alter table train_data add column failure_id integer')\n",
    "    cur.execute('alter table train_data add column pseudo_label real;')\n",
    "\n",
    "    # Load Failure ID column\n",
    "    cur.execute(\"update train_data set failure_id = 1 where '2022-02-28 21:53:00' <= ts and ts < '2022-03-01 02:00:00';\")\n",
    "    cur.execute(\"update train_data set failure_id = 2 where '2022-03-23 14:54:00' <= ts and ts < '2022-03-23 15:24:00';\")\n",
    "    cur.execute(\"update train_data set failure_id = 3 where '2022-05-30 12:00:00' <= ts and ts < '2022-06-02 06:18:00';\")\n",
    "\n",
    "    # Load Pseudo-Label column\n",
    "    cur.execute(\"update train_data set pseudo_label = 1 where '2022-02-21 06:00:00' <= ts and ts < '2022-02-28 02:00:00';\")\n",
    "    cur.execute(\"update train_data set pseudo_label = 2 where '2022-03-16 06:00:00' <= ts and ts < '2022-03-23 02:00:00';\")\n",
    "    cur.execute(\"update train_data set pseudo_label = 3 where '2022-05-23 06:00:00' <= ts and ts < '2022-05-30 02:00:00';\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28601da6",
   "metadata": {},
   "source": [
    "## Create Vector Database\n",
    "Load PDF text into a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d53af8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be5ac3ef-1cac-4849-b4fc-3bb826497d52', '13e9cb95-4b6e-4e93-acb0-c720d31d7bab', 'bb87c475-0e08-4b06-a651-fe05f95ac343', '03808692-57cc-4fa4-ad0b-d3d62e7258c3', '4f594858-6fb8-40ff-ae76-2071ef95dd3a', '32a67021-bd19-48a9-8700-3c4472caaf95', '180145ef-4b54-42c9-a4f1-1204ccfdfabc', '1e5914c9-cf32-4dc5-b68d-11d48e02439e', '36e0602b-ff8e-4a39-9ba8-2da494cab0f0', 'af279618-b1ad-4399-9ef7-7cd3c653d7f1', 'ecaa39d8-2336-4e6e-929c-63d4adb2235c', '192dfe39-2d8d-402e-8918-a442269eb5b7', 'b444115c-74dc-4749-9fcc-456342497265', '33fe7c0a-f55f-450e-9ae0-a5910039ba6d', 'c27092bb-ff81-4a82-a14a-ce1bc2abfce8', 'e6f16af0-20b5-4bef-872a-72fe7ad65ffb', 'f65df40a-dc1d-4805-b18b-f0bd79dca54d', '4201c19b-b330-45f8-a679-c0610bd82f71', '48ca4518-6ab5-4971-a24c-b6ffad73ddac', 'b150a373-2ee9-418c-9c9c-876ce007895a', 'e464fb45-963a-42da-b08e-3644c6ebfb5b', 'fb41a688-2fa0-4086-b603-1c1618c3889d', 'b073819c-d0b8-4ebf-a293-d4b9393b041a', '67cd58ed-bc84-40cf-9f68-c9c42af7d29c', '2294be9d-9978-451a-8ee5-7ca05a6a3d3c', '8c5c7b3c-cace-4d02-b596-888441c16999', '8434979f-589e-4ff1-958a-454aedc9bb4a', 'd64dc893-255f-4fc5-96fb-a8c0f5a9dc1d']\n",
      "CPU times: user 468 ms, sys: 65.1 ms, total: 533 ms\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load PDF\n",
    "# There's only 1 PDF but theoretically, this code can be extended to multiple PDFs\n",
    "loader = PyPDFLoader(r'./01-docs/s41597-022-01877-3.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "# Split PDFs into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Generate & save Vector Database\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "vector_store = Chroma(\n",
    "    collection_name='train_metadata',\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory='./02-data/chroma_db'\n",
    ")\n",
    "\n",
    "ids = vector_store.add_documents(splits)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf006e2",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "Train Prophet models (sequentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4175003, 23)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        select * \n",
    "        from train_data\n",
    "        where \n",
    "            failure_id is null\n",
    "            and '{train_start}' <= ts and ts < '{end}'\n",
    "    \"\"\", con, parse_dates=['ts'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e435102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]15:10:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:22:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 12%|█▎        | 1/8 [14:25<1:41:01, 865.87s/it]15:24:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:33:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 25%|██▌       | 2/8 [25:17<1:13:59, 739.92s/it]15:35:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 38%|███▊      | 3/8 [44:36<1:17:36, 931.31s/it]15:54:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 50%|█████     | 4/8 [47:09<41:35, 623.83s/it]  15:56:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 62%|██████▎   | 5/8 [1:11:31<46:18, 926.33s/it]16:21:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 75%|███████▌  | 6/8 [1:31:30<33:57, 1018.94s/it]16:41:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 88%|████████▊ | 7/8 [1:43:01<15:11, 911.67s/it] 16:52:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "100%|██████████| 8/8 [2:02:21<00:00, 917.71s/it]\n"
     ]
    }
   ],
   "source": [
    "for sensor in tqdm(analog_sensors):\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=0.5, # Lower changepoint prior\n",
    "        yearly_seasonality=False,    # Use custom seasonalities instead \n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False\n",
    "    )\n",
    "\n",
    "    model.add_seasonality(name='30min', period=60*30, fourier_order=5) # Split day in 30min intervals\n",
    "    model.add_seasonality(name='daily', period=72000, fourier_order=5) # 6am - 2am\n",
    "\n",
    "    model.fit(df.loc[:, ['ts', sensor]].rename(columns={'ts': 'ds', sensor: 'y'}))\n",
    "\n",
    "    dump(model, f'03-models/prophet_{sensor}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754361b",
   "metadata": {},
   "source": [
    "## Save Predictions\n",
    "Load model predictions into SQLite3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "724d1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [05:51<00:00, 43.99s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1_000_000\n",
    "\n",
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Create results table\n",
    "    cur.execute('drop table if exists results;') \n",
    "    cur.execute('''\n",
    "        create table results (\n",
    "            ts timestamp,\n",
    "            sensor text,\n",
    "            yhat real,\n",
    "            yhat_lower real,\n",
    "            yhat_upper real,\n",
    "            yhat_lower_with_buffer real,\n",
    "            yhat_upper_with_buffer real,\n",
    "            pred integer\n",
    "        );\n",
    "    ''')\n",
    "\n",
    "    # Calculate inference df\n",
    "    inference_df = pd.read_sql(f\"\"\"\n",
    "        select * \n",
    "        from train_data\n",
    "        where \n",
    "            '{inference_start}' <= ts and ts < '{end}'\n",
    "    \"\"\", con, parse_dates=['ts'])\n",
    "\n",
    "    # Load model and predict\n",
    "    for sensor in tqdm(analog_sensors):\n",
    "        model = load(f'03-models/prophet_{sensor}.joblib')\n",
    "\n",
    "        for start in range(0, len(inference_df), chunk_size):\n",
    "            chunk = inference_df.iloc[start:start + chunk_size]\n",
    "\n",
    "            pred_df = (\n",
    "                model\n",
    "                    .predict(chunk.loc[:, ['ts', sensor]].rename(columns={'ts': 'ds'}))\n",
    "                    .merge(chunk.loc[:, ['ts', sensor]], left_on='ds', right_on='ts')\n",
    "            )\n",
    "\n",
    "            pred_df['sensor'] = sensor\n",
    "            pred_df['range'] = pred_df['yhat_upper'] - pred_df['yhat_lower']\n",
    "            pred_df['range_expanded'] = pred_df['range'] * (1 + out_of_bounds_buffer)\n",
    "            pred_df['yhat_lower_with_buffer'] = pred_df['yhat'] - pred_df['range_expanded'] / 2\n",
    "            pred_df['yhat_upper_with_buffer'] = pred_df['yhat'] + pred_df['range_expanded'] / 2\n",
    "\n",
    "            pred_df['pred'] = (\n",
    "                (pred_df[sensor] < pred_df['yhat_lower_with_buffer']) |\n",
    "                (pred_df[sensor] > pred_df['yhat_upper_with_buffer'])\n",
    "            ).astype(int)\n",
    "\n",
    "            columns = [\n",
    "                'ts',\n",
    "                'sensor',\n",
    "                'yhat',\n",
    "                'yhat_lower',\n",
    "                'yhat_upper',\n",
    "                'yhat_lower_with_buffer',\n",
    "                'yhat_upper_with_buffer',\n",
    "                'pred'\n",
    "            ]\n",
    "            \n",
    "            pred_df[columns].to_sql(\n",
    "                name='results',\n",
    "                con=con, \n",
    "                if_exists='append', \n",
    "                index=False,\n",
    "                chunksize=999,\n",
    "                method='multi'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7b879",
   "metadata": {},
   "source": [
    "## Post-Process Predictions\n",
    "### Aggregate Predictions\n",
    "Individual model results from the previous section are loaded into a flat table. This section pivots the data in order to calculate the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e25b4698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 374 ms, total: 2.79 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute('drop table if exists results_agg;')\n",
    "\n",
    "    cur.execute(\"\"\"create table if not exists results_agg (ts timestamp, {cols}, {agg_cols})\"\"\".format(\n",
    "        cols = ', '.join([f'{sensor}_pred integer' for sensor in analog_sensors]),\n",
    "        agg_cols = 'total_sum integer, pred integer'\n",
    "    ))\n",
    "\n",
    "    cur.execute('''\n",
    "        insert into results_agg (ts, {cols})\n",
    "        select ts, {sum_cols}\n",
    "        from results\n",
    "        group by ts\n",
    "    '''.format(\n",
    "        cols = ', '.join([f'{sensor}_pred' for sensor in analog_sensors]),\n",
    "        sum_cols = ', '.join([f\"sum(case when sensor='{sensor}' then pred else 0 end) as {sensor}_pred\" for sensor in analog_sensors])\n",
    "    ))\n",
    "\n",
    "    cur.execute('''update results_agg set total_sum = {total_sum_col}'''.format(\n",
    "        total_sum_col = ' + '.join([f'{sensor}_pred' for sensor in analog_sensors])\n",
    "    ))\n",
    "\n",
    "    cur.execute(f'''update results_agg set pred = case when total_sum >= {n_signal_threshold} then 1 else 0 end''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcc6c0",
   "metadata": {},
   "source": [
    "### Event Consolidation\n",
    "This section consolidates short events, e.g. you can have two 5 second events that are 4 seconds apart. This section would consolidate those two events into a single 14 second event in order to reduce the amount of noise in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa0f62c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 906 ms, sys: 113 ms, total: 1.02 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # Create initial anomalies table\n",
    "    cur.execute('drop table if exists anomalies')\n",
    "    cur.execute(\"\"\"\n",
    "        create table anomalies (\n",
    "            event_id int,\n",
    "            start_ts timestamp,\n",
    "            end_ts timestamp,\n",
    "            event_duration_in_secs int\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(f\"\"\"\n",
    "        with \n",
    "        lagged as (\n",
    "            select\n",
    "                ts\n",
    "                ,pred\n",
    "                ,lag(pred, 1, 0) over (order by ts asc) as prev_pred\n",
    "            from results_agg\n",
    "        ),\n",
    "        flagged as (\n",
    "            select\n",
    "                ts\n",
    "                ,pred\n",
    "                ,sum(case when pred != prev_pred then 1 else 0 end) over (order by ts rows unbounded preceding) as event_id\n",
    "            from lagged\n",
    "        )\n",
    "        insert into anomalies (event_id, start_ts, end_ts, event_duration_in_secs)\n",
    "        select\n",
    "            event_id\n",
    "            ,min(ts) as start_ts\n",
    "            ,max(ts) as end_ts\n",
    "            ,count(*) as event_duration_in_secs\n",
    "        from flagged\n",
    "        where pred = 1\n",
    "        group by event_id\n",
    "        order by start_ts asc\n",
    "    \"\"\")\n",
    "\n",
    "    # Consolidate events\n",
    "    events = pd.read_sql(\"\"\"\n",
    "        select \n",
    "            *\n",
    "            ,null as gap\n",
    "            ,null as new_event_id \n",
    "        from anomalies\n",
    "    \"\"\", con, parse_dates=['start_ts', 'end_ts'])\n",
    "\n",
    "    event_id = 1\n",
    "\n",
    "    for i in range(events.shape[0]):\n",
    "        if i == 0:\n",
    "            events.loc[i, 'new_event_id'] = event_id\n",
    "            continue\n",
    "\n",
    "        event_gap = (events.loc[i, 'start_ts'] - events.loc[i - 1, 'end_ts']).seconds - 1\n",
    "        if event_gap > event_consolidation_threshold_secs:\n",
    "            event_id += 1\n",
    "\n",
    "        else:\n",
    "            events.loc[i, 'gap'] = event_gap\n",
    "\n",
    "        events.loc[i, 'new_event_id'] = event_id\n",
    "\n",
    "    events.to_sql(\n",
    "        name='temp',\n",
    "        con=con, \n",
    "        if_exists='replace', \n",
    "        index=False,\n",
    "        chunksize=999,\n",
    "        method='multi'\n",
    "    )\n",
    "\n",
    "    cur.execute('drop table if exists anomalies_consolidated;')\n",
    "    cur.execute(f\"\"\"\n",
    "        create table anomalies_consolidated as\n",
    "            select\n",
    "                new_event_id as event_id\n",
    "                ,min(start_ts) as start_ts\n",
    "                ,max(end_ts) as end_ts\n",
    "                ,sum(event_duration_in_secs) + sum(gap) as event_duration_in_secs\n",
    "            from temp\n",
    "            group by new_event_id\n",
    "            order by start_ts asc\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute('drop table temp;')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67628f",
   "metadata": {},
   "source": [
    "### Create Tables for LLM Ingestion\n",
    "This section creates the tables that are used for `01_inference.ipynb` as well as the Streamlit app in `04-app`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d351732",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    df = pd.read_sql(\"\"\"\n",
    "        select * \n",
    "        from anomalies_consolidated \n",
    "        where event_duration_in_secs >= 60*5\n",
    "    \"\"\", con, parse_dates=['start_ts', 'end_ts'])\n",
    "\n",
    "    cur = con.cursor()\n",
    "    cur.execute('drop table if exists train_data_processed;')\n",
    "    cur.execute(\"\"\"\n",
    "        create table train_data_processed as\n",
    "            select \n",
    "                td.*\n",
    "                ,ra.tp2_pred\n",
    "                ,ra.tp3_pred\n",
    "                ,ra.h1_pred\n",
    "                ,ra.dv_pressure_pred\n",
    "                ,ra.reservoirs_pred\n",
    "                ,ra.oil_temperature_pred\n",
    "                ,ra.flowmeter_pred\n",
    "                ,ra.motor_current_pred\n",
    "                ,ra.total_sum\n",
    "                ,ra.pred\n",
    "                ,0 as pred_filtered\n",
    "                \n",
    "            from train_data as td\n",
    "                \n",
    "                left join results_agg as ra \n",
    "                    on td.ts = ra.ts\n",
    "    \"\"\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cur.execute(f\"\"\"\n",
    "            update train_data_processed\n",
    "            set pred_filtered = 1\n",
    "            where '{row['start_ts']}' <= ts and ts <= '{row['end_ts']}';\n",
    "        \"\"\")\n",
    "\n",
    "    cur.execute('create index if not exists idx_tdp_ts on train_data_processed(ts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5e4d2428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>start_ts</th>\n",
       "      <th>end_ts</th>\n",
       "      <th>event_duration_in_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>2022-02-27 08:53:55</td>\n",
       "      <td>2022-02-27 09:02:06</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id            start_ts              end_ts  event_duration_in_secs\n",
       "0       576 2022-02-27 08:53:55 2022-02-27 09:02:06                     492"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62666ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentinel-devices",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

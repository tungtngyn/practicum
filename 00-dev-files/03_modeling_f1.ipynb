{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b35c79",
   "metadata": {},
   "source": [
    "# Outlier Detection Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6343c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.dif import DIF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.copod import COPOD\n",
    "\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4307068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4383806, 24)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    df = pd.read_sql(\"select * from train_data where date(ts) <= '2022-03-02' order by ts asc\", con=con, parse_dates=['ts'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78723973",
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_cols = [\n",
    "    'tp2',\n",
    "    'tp3',\n",
    "    'h1',\n",
    "    'dv_pressure',\n",
    "    'reservoirs',\n",
    "    'oil_temperature',\n",
    "    'flowmeter',\n",
    "    'motor_current'\n",
    "]\n",
    "\n",
    "digital_cols = [\n",
    "    'comp',\n",
    "    'dv_electric',\n",
    "    'towers',\n",
    "    'mpg',\n",
    "    'lps',\n",
    "    'pressure_switch',\n",
    "    'oil_level',\n",
    "    'caudal_impulses'\n",
    "]\n",
    "\n",
    "gps_cols = [\n",
    "    'gps_long',\n",
    "    'gps_lat',\n",
    "    'gps_speed',\n",
    "    'gps_quality'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88ee608",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute('drop table if exists pyod_results;')\n",
    "\n",
    "    cur.execute('''\n",
    "        create table pyod_results (\n",
    "            ts timestamp,\n",
    "            model text,\n",
    "            p_normal real,\n",
    "            p_outlier real,\n",
    "            confidence real,\n",
    "            pred int\n",
    "        )\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a04aa",
   "metadata": {},
   "source": [
    "## Prophet-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da606895",
   "metadata": {},
   "source": [
    "### Prophet - Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col in analog_cols:\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=0.5,\n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False\n",
    "    )\n",
    "\n",
    "    model.add_seasonality(name='30min', period=60*30, fourier_order=5)\n",
    "    model.add_seasonality(name='daily', period=72000, fourier_order=5)\n",
    "\n",
    "    model.fit(df.loc[:, ['ts', col]].rename(columns={'ts': 'ds', col: 'y'}))\n",
    "\n",
    "    dump(model, f'models/prophet_{col}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb1842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [1:29:13<00:00, 669.23s/it] \n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1_000_000\n",
    "\n",
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute('drop table if exists prophet_results;') \n",
    "    cur.execute('''\n",
    "        create table prophet_results (\n",
    "            ds timestamp,\n",
    "            col text,\n",
    "            yhat real,\n",
    "            yhat_lower real,\n",
    "            yhat_upper real,\n",
    "            yhat_lower_expanded real,\n",
    "            yhat_upper_expanded real,\n",
    "            pred integer\n",
    "        );\n",
    "    ''')\n",
    "\n",
    "    for col in tqdm(analog_cols):\n",
    "        model = load(f'models/prophet_{col}.joblib')\n",
    "\n",
    "        for start in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[start:start + chunk_size]\n",
    "\n",
    "            pred_df = (\n",
    "                model\n",
    "                    .predict(chunk.loc[:, ['ts', col]].rename(columns={'ts': 'ds'}))\n",
    "                    .merge(chunk.loc[:, ['ts', col]], left_on='ds', right_on='ts')\n",
    "            )\n",
    "\n",
    "            pred_df['col'] = col\n",
    "            pred_df['range'] = pred_df['yhat_upper'] - pred_df['yhat_lower']\n",
    "            pred_df['range_expanded'] = pred_df['range'] * 1.1\n",
    "            pred_df['yhat_upper_expanded'] = pred_df['yhat'] + pred_df['range_expanded'] / 2\n",
    "            pred_df['yhat_lower_expanded'] = pred_df['yhat'] - pred_df['range_expanded'] / 2\n",
    "\n",
    "            pred_df['pred'] = (\n",
    "                (pred_df[col] > pred_df['yhat_upper_expanded']) |\n",
    "                (pred_df[col] < pred_df['yhat_lower_expanded'])\n",
    "            ).astype(int)\n",
    "\n",
    "            sql_cols = [\n",
    "                'ds',\n",
    "                'col',\n",
    "                'yhat',\n",
    "                'yhat_lower',\n",
    "                'yhat_upper',\n",
    "                'yhat_lower_expanded',\n",
    "                'yhat_upper_expanded',\n",
    "                'pred'\n",
    "            ]\n",
    "            \n",
    "            pred_df[sql_cols].to_sql(\n",
    "                name='prophet_results',\n",
    "                con=con, \n",
    "                if_exists='append', \n",
    "                index=False,\n",
    "                chunksize=999,\n",
    "                method='multi'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed5520",
   "metadata": {},
   "source": [
    "### Prophet Results Aggregation - Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ededc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = 5\n",
    "\n",
    "# Post process results\n",
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute('drop table if exists prophet_results_agg;')\n",
    "\n",
    "    cur.execute('''create table if not exists prophet_results_agg (ds timestamp, {cols}, {agg_cols});'''.format(\n",
    "        cols = ', '.join([f'{col}_pred integer' for col in analog_cols]),\n",
    "        agg_cols = 'total_sum integer, pred integer'\n",
    "    ))\n",
    "\n",
    "    cur.execute('''\n",
    "        insert into prophet_results_agg (ds, {cols})\n",
    "        select ds, {sum_cols}\n",
    "        from prophet_results\n",
    "        group by ds\n",
    "    '''.format(\n",
    "        cols = ', '.join([f'{col}_pred' for col in analog_cols]),\n",
    "        sum_cols = ', '.join([f\"sum(case when col='{col}' then pred else 0 end) as {col}_pred\" for col in analog_cols])\n",
    "    ))\n",
    "\n",
    "    cur.execute('''update prophet_results_agg set total_sum = {total_sum_col}'''.format(\n",
    "        total_sum_col = ' + '.join([f'{col}_pred' for col in analog_cols])\n",
    "    ))\n",
    "\n",
    "    cur.execute(f'''update prophet_results_agg set pred = case when total_sum >= {n_signals} then 1 else 0 end''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b59fe",
   "metadata": {},
   "source": [
    "### Prophet - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:38:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/prophet_reservoirs_mv.joblib']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Prophet(\n",
    "    changepoint_prior_scale=0.5,\n",
    "    yearly_seasonality=False,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False\n",
    ")\n",
    "\n",
    "model.add_seasonality(name='30min', period=60*30, fourier_order=5)\n",
    "model.add_seasonality(name='daily', period=72000, fourier_order=5)\n",
    "\n",
    "for col in analog_cols + digital_cols:\n",
    "    if col != 'reservoirs':\n",
    "        model.add_regressor(col)\n",
    "\n",
    "model.fit(df.rename(columns={'ts': 'ds', 'reservoirs': 'y'}))\n",
    "\n",
    "dump(model, f'models/prophet_reservoirs_mv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1_000_000\n",
    "\n",
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"delete from prophet_results where col = 'reservoirs_mv'\")\n",
    "    \n",
    "    model = load(f'models/prophet_reservoirs_mv.joblib')\n",
    "\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "\n",
    "        pred_df = (\n",
    "            model\n",
    "                .predict(chunk.rename(columns={'ts': 'ds'}))\n",
    "                .merge(chunk, left_on='ds', right_on='ts')\n",
    "        )\n",
    "\n",
    "        pred_df['col'] = 'reservoirs_mv'\n",
    "        pred_df['range'] = pred_df['yhat_upper'] - pred_df['yhat_lower']\n",
    "        pred_df['range_expanded'] = pred_df['range'] * 1.1\n",
    "        pred_df['yhat_upper_expanded'] = pred_df['yhat'] + pred_df['range_expanded'] / 2\n",
    "        pred_df['yhat_lower_expanded'] = pred_df['yhat'] - pred_df['range_expanded'] / 2\n",
    "\n",
    "        pred_df['pred'] = (\n",
    "            (pred_df['reservoirs'] > pred_df['yhat_upper_expanded']) |\n",
    "            (pred_df['reservoirs'] < pred_df['yhat_lower_expanded'])\n",
    "        ).astype(int)\n",
    "\n",
    "        sql_cols = [\n",
    "            'ds',\n",
    "            'col',\n",
    "            'yhat',\n",
    "            'yhat_lower',\n",
    "            'yhat_upper',\n",
    "            'yhat_lower_expanded',\n",
    "            'yhat_upper_expanded',\n",
    "            'pred'\n",
    "        ]\n",
    "        \n",
    "        pred_df[sql_cols].to_sql(\n",
    "            name='prophet_results',\n",
    "            con=con, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            chunksize=999,\n",
    "            method='multi'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b3f6b",
   "metadata": {},
   "source": [
    "### Prophet - Multivariate + Multiple Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea10fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]12:01:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:09:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 12%|█▎        | 1/8 [10:23<1:12:46, 623.80s/it]12:12:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:25:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 25%|██▌       | 2/8 [26:40<1:23:08, 831.37s/it]12:28:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:44:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 38%|███▊      | 3/8 [45:31<1:20:41, 968.35s/it]12:47:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:52:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 50%|█████     | 4/8 [53:24<51:30, 772.62s/it]  12:55:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 62%|██████▎   | 5/8 [1:42:27<1:17:46, 1555.39s/it]13:44:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:45:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 75%|███████▌  | 6/8 [2:46:28<1:17:44, 2332.38s/it]14:48:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:58:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      " 88%|████████▊ | 7/8 [2:59:13<30:19, 1819.90s/it]  15:00:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:22:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "100%|██████████| 8/8 [3:23:33<00:00, 1526.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 12s, sys: 55.7 s, total: 21min 8s\n",
      "Wall time: 3h 23min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in tqdm(analog_cols):\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=0.5,\n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False\n",
    "    )\n",
    "\n",
    "    model.add_seasonality(name='30min', period=60*30, fourier_order=5)\n",
    "    model.add_seasonality(name='daily', period=72000, fourier_order=5)\n",
    "\n",
    "    # For every univariate model, use all other columns as additional regressors\n",
    "    for regressor in analog_cols + digital_cols:\n",
    "        if regressor != col:\n",
    "            model.add_regressor(regressor)\n",
    "\n",
    "    model.fit(df.rename(columns={'ts': 'ds', col: 'y'}))\n",
    "\n",
    "    dump(model, f'models/prophet_mv_multi_{col}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5722faa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [1:05:31<00:00, 491.41s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1_000_000\n",
    "\n",
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute('drop table if exists prophet_results_mv_multi;') \n",
    "    cur.execute('''\n",
    "        create table prophet_results_mv_multi (\n",
    "            ds timestamp,\n",
    "            col text,\n",
    "            yhat real,\n",
    "            yhat_lower real,\n",
    "            yhat_upper real,\n",
    "            yhat_lower_expanded real,\n",
    "            yhat_upper_expanded real,\n",
    "            pred integer\n",
    "        );\n",
    "    ''')\n",
    "\n",
    "    for col in tqdm(analog_cols):\n",
    "        model = load(f'models/prophet_mv_multi_{col}.joblib')\n",
    "\n",
    "        for start in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[start:start + chunk_size]\n",
    "\n",
    "            pred_df = (\n",
    "                model\n",
    "                    .predict(chunk.rename(columns={'ts': 'ds'}))\n",
    "                    .merge(chunk, left_on='ds', right_on='ts')\n",
    "            )\n",
    "\n",
    "            pred_df['col'] = col\n",
    "            pred_df['range'] = pred_df['yhat_upper'] - pred_df['yhat_lower']\n",
    "            pred_df['range_expanded'] = pred_df['range'] * 1.1\n",
    "            pred_df['yhat_upper_expanded'] = pred_df['yhat'] + pred_df['range_expanded'] / 2\n",
    "            pred_df['yhat_lower_expanded'] = pred_df['yhat'] - pred_df['range_expanded'] / 2\n",
    "\n",
    "            pred_df['pred'] = (\n",
    "                (pred_df[col] > pred_df['yhat_upper_expanded']) |\n",
    "                (pred_df[col] < pred_df['yhat_lower_expanded'])\n",
    "            ).astype(int)\n",
    "\n",
    "            sql_cols = [\n",
    "                'ds',\n",
    "                'col',\n",
    "                'yhat',\n",
    "                'yhat_lower',\n",
    "                'yhat_upper',\n",
    "                'yhat_lower_expanded',\n",
    "                'yhat_upper_expanded',\n",
    "                'pred'\n",
    "            ]\n",
    "            \n",
    "            pred_df[sql_cols].to_sql(\n",
    "                name='prophet_results_mv_multi',\n",
    "                con=con, \n",
    "                if_exists='append', \n",
    "                index=False,\n",
    "                chunksize=999,\n",
    "                method='multi'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea01874",
   "metadata": {},
   "source": [
    "### Prophet Results Aggregation - Multivariate + Multiple Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48488b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = 5\n",
    "\n",
    "# Post process results\n",
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute('drop table if exists prophet_results_mv_multi_agg;')\n",
    "\n",
    "    cur.execute('''create table if not exists prophet_results_mv_multi_agg (ds timestamp, {cols}, {agg_cols});'''.format(\n",
    "        cols = ', '.join([f'{col}_pred integer' for col in analog_cols]),\n",
    "        agg_cols = 'total_sum integer, pred integer'\n",
    "    ))\n",
    "\n",
    "    cur.execute('''\n",
    "        insert into prophet_results_mv_multi_agg (ds, {cols})\n",
    "        select ds, {sum_cols}\n",
    "        from prophet_results_mv_multi\n",
    "        group by ds\n",
    "    '''.format(\n",
    "        cols = ', '.join([f'{col}_pred' for col in analog_cols]),\n",
    "        sum_cols = ', '.join([f\"sum(case when col='{col}' then pred else 0 end) as {col}_pred\" for col in analog_cols])\n",
    "    ))\n",
    "\n",
    "    cur.execute('''update prophet_results_mv_multi_agg set total_sum = {total_sum_col}'''.format(\n",
    "        total_sum_col = ' + '.join([f'{col}_pred' for col in analog_cols])\n",
    "    ))\n",
    "\n",
    "    cur.execute(f'''update prophet_results_mv_multi_agg set pred = case when total_sum >= {n_signals} then 1 else 0 end''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c401c",
   "metadata": {},
   "source": [
    "## pyOD Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78975f18",
   "metadata": {},
   "source": [
    "### ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "998121ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.9s remaining:   11.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 5.15 s, total: 7.09 s\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ecod.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "m_ecod = ECOD(contamination=0.02, n_jobs=8)\n",
    "m_ecod.fit(df.loc[:, analog_cols + digital_cols])\n",
    "\n",
    "dump(m_ecod, 'models/ecod.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae7523ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    5.1s remaining:   15.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.2s remaining:    9.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.3s remaining:   13.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.5s remaining:   13.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.4s remaining:   13.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.9s remaining:   11.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.3s remaining:    6.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "CPU times: user 2h 3min 43s, sys: 13min 26s, total: 2h 17min 10s\n",
      "Wall time: 2h 17min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = 'ecod'\n",
    "\n",
    "m = load(f'models/{name}.joblib')\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "for start in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[start:start + chunk_size]\n",
    "        \n",
    "    results_df = chunk.loc[:, ['ts']].copy(deep=True)\n",
    "\n",
    "    print('Predicting..')\n",
    "    results_df[['p_normal', 'p_outlier']], results_df['confidence'] = m.predict_proba(chunk.loc[:, analog_cols + digital_cols], return_confidence=True)\n",
    "    results_df['pred'] = results_df['p_outlier'].round().astype(int)\n",
    "    results_df['model'] = name\n",
    "\n",
    "    print('Uploading..')\n",
    "    with sqlite3.connect('./data/data.db') as con:\n",
    "        results_df.to_sql(\n",
    "            name='pyod_results',\n",
    "            con=con, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            chunksize=999,\n",
    "            method='multi'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66945e35",
   "metadata": {},
   "source": [
    "### ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c556f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14h 34min 53s, sys: 2min 44s, total: 14h 37min 38s\n",
      "Wall time: 2h 18min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/abod.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "m_abod = ABOD(contamination=0.02, n_neighbors=20, method='fast')\n",
    "m_abod.fit(df.loc[:, analog_cols + digital_cols])\n",
    "\n",
    "dump(m_abod, 'models/abod.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc8e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "CPU times: user 3h 45min 16s, sys: 12min 18s, total: 3h 57min 34s\n",
      "Wall time: 3h 57min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = 'abod'\n",
    "\n",
    "m = load(f'models/{name}.joblib')\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "for start in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[start:start + chunk_size]\n",
    "        \n",
    "    results_df = chunk.loc[:, ['ts']].copy(deep=True)\n",
    "\n",
    "    print('Predicting..')\n",
    "    results_df[['p_normal', 'p_outlier']], results_df['confidence'] = m.predict_proba(chunk.loc[:, analog_cols + digital_cols], return_confidence=True)\n",
    "    results_df['pred'] = results_df['p_outlier'].round().astype(int)\n",
    "    results_df['model'] = name\n",
    "\n",
    "    print('Uploading..')\n",
    "    with sqlite3.connect('./data/data.db') as con:\n",
    "        results_df.to_sql(\n",
    "            name='pyod_results',\n",
    "            con=con, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            chunksize=999,\n",
    "            method='multi'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017ea4f",
   "metadata": {},
   "source": [
    "### COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0450b9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.5s remaining:   10.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 5.17 s, total: 7.05 s\n",
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/copod.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "m_copod = COPOD(contamination=0.02, n_jobs=8)\n",
    "m_copod.fit(df.loc[:, analog_cols + digital_cols])\n",
    "\n",
    "dump(m_copod, 'models/copod.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086adeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.6s remaining:   13.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.1s remaining:   12.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.7s remaining:    8.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.1s remaining:   12.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.8s remaining:    8.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    4.4s remaining:   13.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "Predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    3.8s remaining:   11.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.4s remaining:    7.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading..\n",
      "CPU times: user 2h 3min 11s, sys: 12min 25s, total: 2h 15min 37s\n",
      "Wall time: 2h 16min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = 'copod'\n",
    "\n",
    "m = load(f'models/{name}.joblib')\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "for start in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[start:start + chunk_size]\n",
    "        \n",
    "    results_df = chunk.loc[:, ['ts']].copy(deep=True)\n",
    "\n",
    "    print('Predicting..')\n",
    "    results_df[['p_normal', 'p_outlier']], results_df['confidence'] = m.predict_proba(chunk.loc[:, analog_cols + digital_cols], return_confidence=True)\n",
    "    results_df['pred'] = results_df['p_outlier'].round().astype(int)\n",
    "    results_df['model'] = name\n",
    "\n",
    "    print('Uploading..')\n",
    "    with sqlite3.connect('./data/data.db') as con:\n",
    "        results_df.to_sql(\n",
    "            name='pyod_results',\n",
    "            con=con, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            chunksize=999,\n",
    "            method='multi'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b15d2",
   "metadata": {},
   "source": [
    "### DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da78279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 9min 39s, sys: 33min 22s, total: 1h 43min 1s\n",
      "Wall time: 1h 37min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/dif.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"'pin_memory' argument is set as true but not supported on MPS\",\n",
    "    category=UserWarning,\n",
    "    module=\"torch.utils.data.dataloader\"\n",
    ")\n",
    "\n",
    "m_dif = DIF(contamination=0.02, random_state=2025)\n",
    "m_dif.fit(df.loc[:, analog_cols + digital_cols])\n",
    "\n",
    "dump(m_dif, 'models/dif.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba21d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "Predicting..\n",
      "Uploading..\n",
      "CPU times: user 3h 49min 56s, sys: 55min 56s, total: 4h 45min 53s\n",
      "Wall time: 4h 32min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = 'dif'\n",
    "\n",
    "m = load(f'models/{name}.joblib')\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "for start in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[start:start + chunk_size]\n",
    "        \n",
    "    results_df = chunk.loc[:, ['ts']].copy(deep=True)\n",
    "\n",
    "    print('Predicting..')\n",
    "    results_df[['p_normal', 'p_outlier']], results_df['confidence'] = m.predict_proba(chunk.loc[:, analog_cols + digital_cols], return_confidence=True)\n",
    "    results_df['pred'] = results_df['p_outlier'].round().astype(int)\n",
    "    results_df['model'] = name\n",
    "\n",
    "    print('Uploading..')\n",
    "    with sqlite3.connect('./data/data.db') as con:\n",
    "        results_df.to_sql(\n",
    "            name='pyod_results',\n",
    "            con=con, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            chunksize=999,\n",
    "            method='multi'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c929da",
   "metadata": {},
   "source": [
    "### pyOD Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae300ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = 5\n",
    "\n",
    "# Post process results\n",
    "with sqlite3.connect('./data/data.db') as con:\n",
    "    cur = con.cursor()\n",
    "\n",
    "    cur.execute('drop table if exists pyod_results_agg;')\n",
    "\n",
    "    cur.execute('''create table if not exists pyod_results_agg (ds timestamp, {cols}, {agg_cols});'''.format(\n",
    "        cols = ', '.join([f'{col}_pred integer' for col in analog_cols]),\n",
    "        agg_cols = 'total_sum integer, pred integer'\n",
    "    ))\n",
    "\n",
    "    cur.execute('''\n",
    "        insert into pyod_results_agg (ds, {cols})\n",
    "        select ds, {sum_cols}\n",
    "        from prophet_results\n",
    "        group by ds\n",
    "    '''.format(\n",
    "        cols = ', '.join([f'{col}_pred' for col in analog_cols]),\n",
    "        sum_cols = ', '.join([f\"sum(case when col='{col}' then pred else 0 end) as {col}_pred\" for col in analog_cols])\n",
    "    ))\n",
    "\n",
    "    cur.execute('''update pyod_results_agg set total_sum = {total_sum_col}'''.format(\n",
    "        total_sum_col = ' + '.join([f'{col}_pred' for col in analog_cols])\n",
    "    ))\n",
    "\n",
    "    cur.execute(f'''update pyod_results_agg set pred = case when total_sum >= {n_signals} then 1 else 0 end''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d72791b",
   "metadata": {},
   "source": [
    "# Sentinel Devices - Anomaly Detection Project\n",
    "\n",
    "ISYE / CSE / MGT 6748\n",
    "\n",
    "Student: Tung Nguyen, tnguyen844@gatech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46dff90",
   "metadata": {},
   "source": [
    "## Distributed Model Training & Synthetic Data\n",
    "\n",
    "This notebook was run on: Python 3.13, macOS 15 Sequoia, Macbook Pro (M1 Max).\n",
    "\n",
    "One of the benefits of a Prophet-based forecasting model is the ability to parallelize model training and model inference, which makes use of modern hardware. Instead of training 8 models sequentially as in `00_setup_and_model_training.ipynb`, the models can be trained in parallel. \n",
    "\n",
    "This notebook utilizes Python's `multiprocessing` library to accomplish this as a proof-of-concept, however, many production-grade workflow orchestrators and machine-learning software have similar features (Ex. [AWS SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html), or [Apache Airflow's MappedTask](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88acb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from prophet import Prophet\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from joblib import dump\n",
    "from utils import train_prophet_model\n",
    "\n",
    "analog_sensors = [\n",
    "    'tp2',\n",
    "    'tp3',\n",
    "    'h1',\n",
    "    'dv_pressure',\n",
    "    'reservoirs',\n",
    "    'oil_temperature',\n",
    "    'flowmeter',\n",
    "    'motor_current'\n",
    "]\n",
    "\n",
    "digital_sensors = [\n",
    "    'comp',\n",
    "    'dv_electric',\n",
    "    'towers',\n",
    "    'mpg',\n",
    "    'lps',\n",
    "    'pressure_switch',\n",
    "    'oil_level',\n",
    "    'caudal_impulses'\n",
    "]\n",
    "\n",
    "gps_sensors = [\n",
    "    'gps_long',\n",
    "    'gps_lat',\n",
    "    'gps_speed',\n",
    "    'gps_quality'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78eec0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count() # Note: you may have less cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7c6b8",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef488a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime(2022, 2, 15, 2, 0, 0) # Simulate \"today\" as Feb 15th\n",
    "\n",
    "training_window = 10 # days -- train on shorter timeframe as a Proof of Concept\n",
    "inference_window = 7 # days\n",
    "\n",
    "n_signal_threshold = 5\n",
    "out_of_bounds_buffer = 0.1\n",
    "\n",
    "event_consolidation_threshold_secs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7d9b5",
   "metadata": {},
   "source": [
    "### Calculated Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2f1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start: 2022-02-05 02:00:00 - 2022-02-15 02:00:00\n",
      "Inference Period: 2022-02-08 02:00:00 - 2022-02-15 02:00:00\n"
     ]
    }
   ],
   "source": [
    "train_start = (today - timedelta(days=training_window)).strftime(r'%Y-%m-%d %H:%M:%S')\n",
    "inference_start = (today - timedelta(days=inference_window)).strftime(r'%Y-%m-%d %H:%M:%S')\n",
    "end = today.strftime(r'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f'Train Start: {train_start} - {end}')\n",
    "print(f'Inference Period: {inference_start} - {end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad2b4e",
   "metadata": {},
   "source": [
    "## Parallel Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6006a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720010, 23)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        select * \n",
    "        from train_data\n",
    "        where \n",
    "            failure_id is null\n",
    "            and '{train_start}' <= ts and ts < '{end}'\n",
    "    \"\"\", con, parse_dates=['ts'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a20f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "19:54:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:54:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:55:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:02:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:02:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:04:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:04:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:05:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:05:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:06:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 185 ms, sys: 509 ms, total: 693 ms\n",
      "Wall time: 11min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = partial(train_prophet_model, df=df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=8) as pool:\n",
    "        _ = pool.map(train, analog_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55186fd",
   "metadata": {},
   "source": [
    "## Synthetic Anomaly Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_multiplier = 1\n",
    "max_multiplier = 1.25 # Max 25% increase\n",
    "\n",
    "n_anomalies = 10\n",
    "i = 0\n",
    "output = []\n",
    "synthetic_df = pd.DataFrame(columns=['row_id', 'multiplier'])\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Sample starting timestamp\n",
    "    # 72_000 seconds is one \"operational\" day, e.g. 6am to 2am @ 1Hz\n",
    "    start_ts = np.random.randint(0, 72_000*training_window) # Random sample within 1 month\n",
    "\n",
    "    # Check if start_ts is within an hour to previously sampled timestamps\n",
    "    if output:\n",
    "        for j in output:\n",
    "            if abs(start_ts - j) <= 60*60:\n",
    "                continue\n",
    "    \n",
    "    # Increment\n",
    "    i += 1\n",
    "\n",
    "    # Random event length, between 10 to 30 mins long\n",
    "    length = np.random.randint(10, 20)*60\n",
    "\n",
    "    # Triangular distribution\n",
    "    half = np.linspace(min_multiplier, max_multiplier, int((length - 1)/2))\n",
    "    full = np.concatenate((half, half[::-1][1:]), axis=0)\n",
    "\n",
    "    # Save samples\n",
    "    temp = pd.DataFrame.from_dict({\n",
    "        'row_id': np.arange(start=start_ts, stop=start_ts + length - 3),\n",
    "        'multiplier': full\n",
    "    })\n",
    "    synthetic_df = (\n",
    "        pd.concat([temp, synthetic_df])\n",
    "        if synthetic_df.shape[0] > 0 \n",
    "        else temp\n",
    "    )\n",
    "\n",
    "    output.append(start_ts)\n",
    "\n",
    "    # Stopping criteria\n",
    "    if i == n_anomalies:\n",
    "        break\n",
    "\n",
    "\n",
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    synthetic_df.to_sql(\n",
    "        name = 'synthetic_anomalies',\n",
    "        if_exists = 'replace',\n",
    "        index = False,\n",
    "        con = con\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bdc4b3",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3539ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720010, 25)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect(r'./02-data/data.db') as con:\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        with\n",
    "        cte as (\n",
    "            select *, row_number() over (order by ts asc) as row_id\n",
    "            from train_data\n",
    "            where \n",
    "                failure_id is null\n",
    "                and '{train_start}' <= ts and ts < '{end}'\n",
    "        )\n",
    "        select c.*, a.multiplier\n",
    "        from cte as c\n",
    "            left join synthetic_anomalies as a\n",
    "                on a.row_id = c.row_id\n",
    "    \"\"\", con, parse_dates=['ts'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfe8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8670 secs out of 720010 secs are anomalies\n",
      "~1.20% of data points\n"
     ]
    }
   ],
   "source": [
    "print(f'{(~df.multiplier.isna()).sum()} secs out of {df.shape[0]} secs are anomalies')\n",
    "print(f'~{(~df.multiplier.isna()).sum()/df.shape[0]:.2%} of data points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc2187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params: dict) -> Prophet:\n",
    "\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=params['changepoint_prior_scale'], \n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False\n",
    "    )\n",
    "\n",
    "    for digital_sensor in [\n",
    "            'comp',\n",
    "            'dv_electric',\n",
    "            'towers',\n",
    "            'mpg',\n",
    "            'lps',\n",
    "            'pressure_switch',\n",
    "            'oil_level',\n",
    "            'caudal_impulses'\n",
    "        ]:\n",
    "        if params[f'regressor_{digital_sensor}']:\n",
    "            model.add_regressor(digital_sensor)\n",
    "\n",
    "    for d in [\n",
    "            {'name': 'seasonality_30min',  'period': 30*60,    'fourier_order': 4},\n",
    "            {'name': 'seasonality_01hour', 'period': 60*60,    'fourier_order': 6},\n",
    "            {'name': 'seasonality_05hour', 'period': 60*60*5,  'fourier_order': 8},\n",
    "            {'name': 'seasonality_10hour', 'period': 60*60*10, 'fourier_order': 10},\n",
    "            {'name': 'seasonality_15hour', 'period': 60*60*15, 'fourier_order': 12},\n",
    "            {'name': 'seasonality_daily',  'period': 60*60*20, 'fourier_order': 14},\n",
    "        ]:\n",
    "        if params[d['name']]:\n",
    "            model.add_seasonality(\n",
    "                name = d['name'], \n",
    "                period = d['period'], \n",
    "                fourier_order = d['fourier_order']    \n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(params: dict, sensor: str, df: pd.DataFrame, batch_size: int = 750_000) -> float:\n",
    "    \n",
    "    # Build & Train Model\n",
    "    model = build_model(params)\n",
    "    model.fit(df.rename(columns={'ts': 'ds', sensor: 'y'}))\n",
    "\n",
    "    # Predict\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        chunk = df.iloc[start:start + batch_size]\n",
    "\n",
    "        pred_df = (\n",
    "            model\n",
    "                .predict(chunk.rename(columns={'ts': 'ds'}))\n",
    "                .merge(chunk, left_on='ds', right_on='ts')\n",
    "        )\n",
    "\n",
    "        pred_df['range'] = pred_df['yhat_upper'] - pred_df['yhat_lower']\n",
    "        pred_df['range_expanded'] = pred_df['range'] * 1.1\n",
    "        pred_df['yhat_upper_expanded'] = pred_df['yhat'] + pred_df['range_expanded'] / 2\n",
    "        pred_df['yhat_lower_expanded'] = pred_df['yhat'] - pred_df['range_expanded'] / 2\n",
    "\n",
    "        pred_df['y_pred'] = (\n",
    "            (pred_df[sensor] > pred_df['yhat_upper_expanded']) |\n",
    "            (pred_df[sensor] < pred_df['yhat_lower_expanded'])\n",
    "        ).astype(int)\n",
    "        \n",
    "        pred_df['y_true'] = (~pred_df['multiplier'].isna()).astype(int)\n",
    "\n",
    "        tp += ((pred_df['y_true'] == 1) & (pred_df['y_pred'] == 1)).sum()\n",
    "        fp += ((pred_df['y_true'] == 0) & (pred_df['y_pred'] == 1)).sum()\n",
    "\n",
    "    # Score\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Log\n",
    "    with open(f'./output/train_{sensor}.log', 'a') as f:\n",
    "        f.write(f'{precision}, {params}\\n')\n",
    "\n",
    "    return {'loss': -precision, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97231f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'changepoint_prior_scale': <hyperopt.pyll.base.Apply at 0x44222b5d0>,\n",
       " 'regressor_comp': <hyperopt.pyll.base.Apply at 0x442054250>,\n",
       " 'regressor_dv_electric': <hyperopt.pyll.base.Apply at 0x4422288d0>,\n",
       " 'regressor_towers': <hyperopt.pyll.base.Apply at 0x442228c50>,\n",
       " 'regressor_mpg': <hyperopt.pyll.base.Apply at 0x442228fd0>,\n",
       " 'regressor_lps': <hyperopt.pyll.base.Apply at 0x442229350>,\n",
       " 'regressor_pressure_switch': <hyperopt.pyll.base.Apply at 0x4422296d0>,\n",
       " 'regressor_oil_level': <hyperopt.pyll.base.Apply at 0x442229a50>,\n",
       " 'regressor_caudal_impulses': <hyperopt.pyll.base.Apply at 0x442229dd0>,\n",
       " 'seasonality_30min': <hyperopt.pyll.base.Apply at 0x44222a150>,\n",
       " 'seasonality_01hour': <hyperopt.pyll.base.Apply at 0x44222a4d0>,\n",
       " 'seasonality_05hour': <hyperopt.pyll.base.Apply at 0x44222a850>,\n",
       " 'seasonality_10hour': <hyperopt.pyll.base.Apply at 0x44222abd0>,\n",
       " 'seasonality_15hour': <hyperopt.pyll.base.Apply at 0x44222af50>,\n",
       " 'seasonality_daily': <hyperopt.pyll.base.Apply at 0x44222b2d0>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:44:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressors = {\n",
    "    f'regressor_{sensor}': hp.choice(f'regressor_{sensor}', [True, False])\n",
    "    for sensor in digital_sensors\n",
    "}\n",
    "\n",
    "seasonalities = {\n",
    "    seasonality: hp.choice(seasonality, [True, False])\n",
    "    for seasonality in [\n",
    "        'seasonality_30min',\n",
    "        'seasonality_01hour',\n",
    "        'seasonality_05hour',\n",
    "        'seasonality_10hour',\n",
    "        'seasonality_15hour',\n",
    "        'seasonality_daily'\n",
    "    ]\n",
    "}\n",
    "\n",
    "space = {\n",
    "    'changepoint_prior_scale': hp.uniform('changepoint_prior_scale', 0.01, 1.0),\n",
    "    **regressors,\n",
    "    **seasonalities\n",
    "}\n",
    "\n",
    "display(space) # display search space\n",
    "\n",
    "f_objective = partial(objective, df=df, sensor='oil_temperature')\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn = f_objective, \n",
    "    space = space, \n",
    "    early_stop_fn = no_progress_loss(10), \n",
    "    max_evals = 100, \n",
    "    rstate = np.random.default_rng(1), \n",
    "    algo = tpe.suggest, \n",
    "    trials = trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452c9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentinel-devices",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
